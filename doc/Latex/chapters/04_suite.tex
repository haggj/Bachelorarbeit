\chapter{Benchmarking Suite}\label{chapter:benchmarking_suite}
In this chapter a benchmarking suite is presented that is able to generate comparable benchmarks between \gls{SIDH} implementations introduced in \autoref{chapter:existing_sidh}. To get a better understanding of the results, this chapter starts with a  description of the tools and methodologies used for benchmarking (\autoref{sec:benchmarks_methods}). The implementation details given in \autoref{sec:benchmarks_details} provide precise internals of the benchmarking suite. This might be used for further development of the software. A description of how to actually use the presented benchmarking suite is given in \autoref{sec:benchmarks_usage}.

The benchmarking suite is available on Github\footnote{\texttt{https://github.com/haggj/Bachelorarbeit}}.

\section{Benchmarking Methodology}\label{sec:benchmarks_methods}
In order to generate independent and stable benchmarking results, the benchmarking suite runs within a virtual environment: \textit{Docker} is used to separate the running suite from the host operating system. This reduces the influence of resource intensive processes which might falsify benchmarking results. Moreover, \textit{Docker} enables a portable and scalable software solution. The benchmarking suite runs within an Ubuntu Docker container providing a virtual operating system.\\
The benchmarks measured in this thesis are \textit{execution time} and \textit{memory consumption}. These measurements are used to quantify the claimed resources of an algorithm. 
\\
In the following, the process of benchmarking is described within five steps. In a summarized version, the required steps are:

\begin{enumerate}
  \itemsep0em 
  \item Create the benchmarking source code
  \item Compile the benchmarking source code
  \item Run \textit{Callgrind}
  \item Run \textit{Massif}
  \item Collect benchmarks
\end{enumerate}

\subsubsection{Create the benchmarking code}
Each software libraries presented in \autoref{chapter:existing_sidh} implements various cryptographic primitives. To avoid overhead when calculating benchmarks, only the required functions that generate a secret key via \gls{SIDH} should be called. Thus, a simple benchmarking file is provided for each implementation which calls the appropriate underlying API. This benchmarking file must ensure that all required headers are imported, the API is called correctly and a \texttt{main}-function is provided.

\subsubsection{Compile the benchmarking code}
Once the benchmarking source code is created, it needs to be compiled to a binary. Therefore, all required dependencies (libraries, headers, source code, ..) need to be provided to the compiler. The benchmarking suite provides a \texttt{Makefile} for each implementation, where the compilation process is implemented.
All compilations performed for the benchmarking suite must use consistent compiler optimizations. This ensures the comparability of the binaries. Currently, the optimization flag \texttt{-O3} is passed to the gcc compiler. Note that CIRCL is implemented in GO and therefore the go compiler is used for compilation. Since this compiler does not provide optimization flags, the default compiler optimizations are used \parencite{gowiki2020compiler}.
To be able to extract benchmarks for specific functions \textit{inlining} is disabled during compilation. In order to avoid the modification of the source code of the libraries simple wrapper function were created, which only call the underlying API function. These wrapper functions are marked to avoid \textit{inlining}. The C programming language offers the following directive to disable \textit{inlining} for a function:
\begin{lstlisting}[language=C]
// This function will not be inlined by the compiler
void __attribute__ ((noinline)) no_inlining() {
// ...
}

// This function might be inlined by the compiler
void inlining() {
// ...
}
\end{lstlisting}
The go compiler can be directly invoked with a no-inlining (\texttt{-l}) flag:
\begin{lstlisting}[language=Go]
go build -gcflags "-l"
\end{lstlisting}


\subsubsection{Run \textit{Callgrind}}
\textit{Callgrind} records function calls of a binary. For each call the executed instructions which are processed by the processor are counted. Moreover, the tool provides detailed information about the callee and frequency of function calls. Thus, the tool also provides information about execution hotspots of the binary. \textit{Callgrind} is part of \textit{Valgrind}, a profiling tool which allows deep analysis of executed binaries.\\
Callgrind is invoked on the command line via:
\begin{lstlisting}[language=Bash]
valgrind --tool=callgrind --callgrind-out-file=callgrind.out binary
\end{lstlisting}
The profiling data of \textit{callgrind} is written to the file defined by \texttt{--callgrind-out-file}. This file might be analyzed using a graphical tool like \textit{KCachegrind} or any other analyzing script.
Running a binary using callgrind slows down the execution times of that binary significantly. This is the main reason for the long execution times when collecting benchmarks. \\\\
Besides counting the instructions executed by the processor, one could also count elapsed \gls{CPU} cycles or equivalently the elapsed time for a concrete binary. In the following paragraph the differences are considered and reasons for measuring instructions are given.
\\
Firstly, the measured instruction count of \textit{callgrind} can be converted to elapsed \gls{CPU} cycles and to thus to the elapsed time. The following calculation exemplarily shows this conversion for the execution of a SIDH key exchange (using the \gls{SIKE} \texttt{x64\_optimized} implementation initialized with parameter set \texttt{p434}). \textit{Callgrind} measures constantly $64\:621\:792$ instructions when executing that binary. The performance analyzing tool \textit{perf} can also be run on that binary (\texttt{sudo run perf \textit{binary}}). Besides the elapsed \gls{CPU} cycles and elapsed time \textit{perf} also measures the executed instructions per \gls{CPU} cycle (\gls{IPC}) and the average \gls{CPU} frequency in GHz. This values can be used to compute the elapsed time of an application. It is important to note, that the \gls{IPC} is not a characteristic index for a \gls{CPU}, moreover it varies between multiple runs of the same application based on scheduling decisions and hardware dependencies (e.g. cache size) ~\parencite{alameldeen2006ipc}. Thus, the measured \gls{CPU} cycles and the corresponding elapsed time of an application is not constant. The measured \gls{IPC} for the binary (\gls{SIKE} \texttt{x64\_optimized} using \texttt{p434}) is on average $2.87$ (10 samples). The measured \gls{CPU} frequency is on average $2.66\;GHz$ (10 samples). Thus, the elapsed time depending on the measured instructions can be computed as follows~\parencite{alameldeen2006ipc}:

\begin{equation*}
\begin{split}
Elapsed\;Time & = \frac{Instructions}{Instructions\;per\;Cycle\;(IPC)\cdot CPU\; Frequency\;in\;GHz\cdot 10^9}\;[s] \\\\
& = \frac{Instructions}{2.87\cdot 2.66\cdot 10^9}\;[s] = \frac{Instructions}{7,6342\cdot 10^9}\;[s] 
\end{split}
\end{equation*}
\\\\For the instructions counted using \textit{callgrind} this formula yields:
\begin{equation*}
\begin{split}
Elapsed\;Time\ & = \frac{Instructions}{7,6342\cdot 10^9} = \frac{64\:621\:792}{7,6342\cdot 10^9} = 0.0084647759\;[s] = 8.465\;[ms]
\end{split}
\end{equation*}

At the same time the measured execution time by \textit{perf} is on average $8.476\;[ms]$ (10 samples). Since the difference is marginal it seems to be sufficient to provide the instruction counts instead of measuring the exact execution times.
\\
Secondly, the elapsed time for an application depends on the \gls{IPC}, which is a highly dynamic value (e.g. depending on scheduling decisions, cache misses or parallelism of the underlying \gls{CPU} ~\parencite{alameldeen2006ipc}). At the same time \textit{callgrind} measures the executed instructions of the compiled application, which is a constant value if the binary itself implements static runtime behavior (e.g. the measured instructions of a binary printing \textit{"Hello World"} once is constant, while the executed instructions of a binary printing a randomly chosen amount of \textit{"Hello World"} messages is not constant). Thus, the measured instructions only depend on the compiler and the \gls{CPU} architecture (\gls{ISA}) and they are independent of the dynamic behavior of the concrete underlying \gls{CPU}. Besides abstracting from the concrete \gls{CPU} this also increases reproducibility of the benchmarking results.

\subsubsection{Run \textit{Massif}}
\textit{Massif} measures memory usage of a binary including heap and stack. \textit{Massif} is also part of the profiling tool \textit{Valgrind}. The tool creates multiple snapshots of the memory consumption during execution. Thus, one can extract the maximum memory consumption of a binary. The following command runs the \textit{Massif}:
\begin{lstlisting}[language=Bash]
valgrind --tool=massif --stacks=yes --massif-out-file=massif.out binary
\end{lstlisting}
The profiling data will be written to the file defined by \texttt{--massif-out-file}. \textit{Massif-visualizer} could be used to graphically analyze the data.

\subsubsection{Collect benchmarks}
Once the output files of \textit{Callgrind} and \textit{Massif} are produced, one can parse and analyze the corresponding files to obtain:
\begin{enumerate}
\item Absolute instructions per function.
\item Maximum memory consumption during \gls{SIDH} key exchange.
\end{enumerate}
This information is finally used by the benchmarking suite to produce graphs and tables for further investigation. To receive reliable information all benchmarks are measured multiple times (e.g. $N=100$ times) and averaged over all outcomes.

\section{Application Details}\label{sec:benchmarks_details}
The benchmarking suite is developed in Python3 on a Linux/Ubuntu operating system. Currently, the following implementations are included:
\label{sec:included_implementations}
\begin{itemize}
\item \gls{SIKE} supporting \texttt{p434, p503, p610 and p751}
	\begin{itemize}
	\item SIKE reference implementation (\texttt{SIKE\_Reference})
	\item SIKE generic optimized implementation (\texttt{SIKE\_Generic})
	\item SIKE generic optimized and compressed implementation (\texttt{SIKE\_Generic\_Compressed})
	\item SIKE x64 optimized implementation (\texttt{SIKE\_x64})
	\item SIKE x64 optimized and compressed implementation (\texttt{SIKE\_x64\_Compressed})
	\end{itemize}
\item \gls{PQCrypto-SIDH} supporting \texttt{p434, p503, p610 and p751}
	\begin{itemize}
	\item PQCrypto-SIDH generic optimized implementation (\texttt{Microsoft\_Generic})
	\item PQCrypto-SIDH generic optimized and compressed implementation \\ (\texttt{Microsoft\_Generic\_Compressed})
	\item PQCrypto-SIDH x64 optimized implementation (\texttt{Microsoft\_x64})
	\item PQCrypto-SIDH x64 optimized and compressed implementation \\ (\texttt{Microsoft\_x64\_Compressed})
	\end{itemize}
\item CIRCL supporting \texttt{p434, p503 and p751}
	\begin{itemize}
	\item CIRCL generic optimized implementation (\texttt{CIRCL\_Generic})
	\item CIRCL x64 optimized implementation (\texttt{CIRCL\_x64})
	\end{itemize}
\end{itemize}
Furthermore, the benchmarking suite provides \gls{ECDH} implemented in \gls{openssl}. Since \gls{SIDH} is a candidate to replace current Diffie-Hellman algorithms, \gls{ECDH} is intended as reference value: The objective is to compare optimized state-of-the-art \gls{ECDH} with quantum-secure \gls{SIDH}.\\
Since each parameter set of \gls{SIDH} meets a different security level (compare \autoref{sidh_security}) \gls{ECDH} is also instantiated with different curves, each matching a \gls{SIDH} security level. The used elliptic curves are:
\begin{enumerate}
\item \texttt{secp256r1} also known as \texttt{P-256} (\gls{openssl} implementation: \texttt{prime256v1} \parencite{turner2009elliptic}): \\128 bit security matching \texttt{p434} \parencite{brown2010sec}
\item \texttt{secp384r1} also known as \texttt{P-384} (\gls{openssl} implementation \texttt{secp384r1}): \\192 bit security matching \texttt{p610} \parencite{brown2010sec}
\item \texttt{secp521r1} also known as \texttt{P-521} (\gls{openssl} implementation \texttt{secp521r1}): \\256 bit security  matching \texttt{p751} \parencite{brown2010sec}
\end{enumerate}

Note that the classical security assumptions for \texttt{p434} (key search for 128 bit AES) and \texttt{p503} (collision search for 256 bit SHA) are comparable: A key search for a 128 bit AES key takes $2^{128}$ operations indicating a security level of $128$ bits \parencite{lenstra2006key}. Due to the birthday attack \parencite{gupta2015birthday}, the collision finding on a 256 bit hash function requires about $2^{256/2} = 2^{128}$ operations leading to a security level of 128 bit \parencite{lenstra2006key}. Thus, both parameter sets \texttt{p434} and \texttt{p503} provide similar classical security levels. NIST maps this security level to elliptic curves employing key sizes of 256 bit (e.g. curve \texttt{secp256r1}). Hence, no separate elliptic curves for \texttt{p434} and \texttt{p503} are included. For completion: Security levels of 192 bit and 256 bit are mapped to elliptic curves providing of key sizes 384 bit (e.g.  \texttt{secp384r1}) and 512 bit (e.g.  \texttt{secp512r1}), respectively \parencite{barker2016recommendation}. 
\\\\
For each of these introduced implementations the application measures benchmarks. The following sections present detailed information about the internals of the suite. Besides the precise application flow (\autoref{sec:app_flow}), the internal class structure of the Python3 code is shown (\autoref{sec:app_structure}).

\subsection{Application Flow}\label{sec:app_flow}
This sections illustrates the application flow of the benchmarking suite (see \autoref{fig:app_flow}). Once triggered to run benchmarks, the following procedure is repeated for every implementation: The suite first compiles the benchmarking code. The binary is then executed multiple times to generate $N$ benchmarking results, respectively for \textit{Callgrind} and \textit{Massif}. Outputs of \textit{Callgrind} and \textit{Massif} are parsed and saved in the benchmarking suite after each run.
\\
Finally, the results are visualized in different formats. Graphs compare the recorded instruction counts and the peak memory consumption among implementations instantiated with comparable security classes. The \gls{HTML} and LaTeX tables lists detailed benchmarks per implementation.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.7\textwidth]{benchmarks/application_flow}
  \caption[Flow chart of the benchmarking suite.]
  {Flow chart of the benchmarking suite. For each implementation, the source code is compiled and benchmarked multiple times. The benchmarking results are visualized in different format: Graphs, \gls{HTML} and LaTeX.
  } \label{fig:app_flow}
\end{figure}




\subsection{Application Structure}\label{sec:app_structure}

This section covers the internal structure of the benchmarking suite. It illustrates, how each implementation is represented in code and how the benchmarking results are managed. To get a detailed description of the implemented functions, consider the highly-documented source code of the benchmarking suite.

\subsubsection{Representation of concrete implementations}

The main logic of the benchmarking suite is placed within the class \texttt{BaseImplementation}. This class implements the logic for compiling, executing \textit{Callgrind} and \textit{Massif} and parsing their results. For each implementation which shall be benchmarked, a subclass of \texttt{BaseImplementation} is created (see \autoref{fig:uml}). These subclasses provide a link to the respective \textit{Makefile} which is used by the \texttt{BaseImplementation} for compiling, running callgrind and running massif. Furthermore, each subclass can provide a set of arguments passed to the \textit{Makefile}.

\begin{figure}[]
  \centering
  \includegraphics[width=0.9\textwidth]{benchmarks/uml}
  \caption[Class diagram for supported implementations]
  {Class diagram for all implementations supported by the benchmarking suite. All concrete implementations of \gls{SIDH} inherit from \texttt{BaseImplementation}. A \textit{Makefile} provided by each subclass specifies how the benchmarking code is built and run.} \label{fig:uml}
\end{figure}

\subsubsection{Representation of benchmarking results}
In order to ensure a clear analysis of the results, the application implements a structure for the returned benchmarking values. Since each implementation can be run based on different parameter sets (in the terminology of the benchmarking suite this is called \textit{curves}) and for each curve different benchmarking values are processed, the hierarchy shown in \autoref{fig:uml_results} is applied.\\
The class \texttt{BenchmarkImpl} represents the benchmarking results of a specific implementation. Therefore, the class manages a list of \texttt{BenchmarkCurve} objects which contains benchmarks for a specific curve. Each benchmark for such a curve is represented by an instance of \texttt{Benchmark}. Thus, \texttt{BenchmarkCurve} holds a list of \texttt{Benchmark} objects.
\begin{figure}[H]
  \centering
  \includegraphics[width=1\textwidth]{benchmarks/uml_results}
  \caption[Class diagram for benchmarking results.]
  {Class diagram representing the management of the benchmarking results.}\label{fig:uml_results}
\end{figure}
\newpage
\subsection{Adding Implementations}\label{sec:benchmarks_details_add}
To add a new implementation to the benchmarking suite it is recommended to consider already existing implementations. Moreover, appendix \ref{app:hierarchy} might be helpful to get a better understanding about the applied project hierarchy. Adding a \gls{SIDH} implementation to the benchmarking suite requires the following steps:
\begin{enumerate}
\itemsep0em 
\item Add a new folder into the \texttt{container/} folder of the root directory \\ \texttt{container/new\_implementation/}.
\item Create a file \texttt{container/new\_implementation/benchmark.c} that calls the \gls{SIDH} key exchange API of the new implementation.
\item Add all necessary dependencies into \texttt{container/new\_implementation/} that are needed to compile \texttt{benchmark.c}. Note, maybe some changes in the \textit{Dockerfile} are necessary to install certain dependencies on the virtual operating system started by Docker.
\item Create a \texttt{Makefile} supporting the following commands:
	\begin{itemize}
		\item build: Compile \texttt{benchmark.c} into the executable \\ \texttt{new\_implementation/build/benchmark}
		\item callgrind: Runs callgrind with the executable and stores the result in\\ \texttt{new\_implementation/benchmark/callgrind.out}.
		\item massif: Runs massif with the executable and stores the result in\\ \texttt{new\_implementation/benchmark/massif.out}.
	\end{itemize}
\item Add a new class to the source code which inherits from \texttt{BaseImplementation}. You need to overwrite the \texttt{map\_functions()} method to map the specific API functions of the new implementation to the naming used within the benchmarking suite. The new class might look similar to this:

\begin{lstlisting}[language=Python]
class New_Implementation(BaseImplementation):
    def __init__(self, count):
        super().__init__(count=count,
				path=[path to Makefile], 
				args=[args for Makefile],
				callgrind_main=[name of main function],
				curves=curves)

    def map_functions(self, callgrind_result: dict) -> dict:
        res = {
            "PrivateKeyA": callgrind_result[[name of api function]],
            "PublicKeyA": callgrind_result[[name of api function]],
            "PrivateKeyB": callgrind_result[[name of api function]],
            "PublicKeyB": callgrind_result[[name of api function]],
            "SecretA": callgrind_result[[name of api function]],
            "SecretB": callgrind_result[[name of api function]],
        }
        return res
\end{lstlisting}
\item Import the created class into \texttt{container/benchmarking.py} and add the class to the \texttt{implementations} list:
\begin{lstlisting}[language=Python]
implementations =[
        #...
        New_Implementation,
    ]
\end{lstlisting}
\end{enumerate}
Once these steps are done the benchmarking suits is able to benchmark the new implementation.

\section{Usage}\label{sec:benchmarks_usage}
This section explains the usage of the benchmarking suite. All tasks need to run within the docker container. Thus, it is mandatory to \href{https://docs.docker.com/get-docker/}{install docker} on your system to run the benchmarking suite\footnote{The application was developed using Docker version \texttt{19.03.8}, build \texttt{afacb8b7f0}. Instructions for installing \textit{Docker}: \url{https://docs.docker.com/get-docker/}}. To provide an easy to use interface a script \textit{run.sh} is available, which might be used to get a better understanding of the required docker commands. This script can be invoked with the following arguments:
\begin{itemize}
\itemsep0em 
\item Building the docker container:
\begin{lstlisting}[numbers=none,linewidth=4cm]
./run.sh build
\end{lstlisting}


\item Running benchmarks within the docker container.
\begin{lstlisting}[numbers=none,linewidth=10cm]
./run.sh benchmark [-N=REPETITIONS] [--no-cache]
\end{lstlisting}
This command triggers the \textit{benchmarking.py} script within the docker container. This script contains the entry logic of the benchmarking suite. It benchmarks all available implementations and generates different output formats (graphs, \gls{HTML}, LaTeX). The frequency of measurements for each benchmark is configurable: The optional argument \textit{-N}  defines the amount of repetitions for \textit{callgrind} and \textit{massif}. Once the benchmarks are done all output files can be inspected in the  \texttt{data/} folder of your current directory. These files visualize average values over N samples. The output files are:
	\begin{itemize}
	\item \textit{XXX.png}: These files compare the absolute instruction counts of all implementations which were run on the XXX parameter set (XXX $\in {434, 503, 610, 751}$). Additionally, it contains a \gls{ECDH} benchmark for comparison.
	\item \textit{XXX\_mem.png}: These files compare the peak memory consumption of all implementations which were run on the XXX parameter set (XXX $\in {434, 503, 610, 751}$). Additionally, it contains a \gls{ECDH} benchmark for comparison.

	\item \textit{cached.json}: This \textit{json} file contains cached benchmarking results. It can be used as input for the benchmarking suite to import cached data instead of generating all benchmarks again. Since the benchmarking suite runs multiple hours if each implementation is evaluated $N$=$100$ times, this functionality speeds up the generation of the output graphs significantly. To use the cached file as input, copy it to \texttt{container/.cached/cached.json} and run the benchmarking suite again. To avoid the use of cached data either delete \texttt{container/.cached/cached.json} or simply invoke the benchmarking suite with the \textit{no-cache} flag.	
	
	\item \textit{results.html}: This file lists detailed benchmarking results in a human readable \gls{HTML} table. 
	
	\item \textit{results.tex}: This file contains the benchmarking results formatted in LaTeX. The output is used for this thesis.
	\end{itemize}

\item Running unit tests within the docker container.
\begin{lstlisting}[numbers=none,linewidth=4cm]
./run.sh test
\end{lstlisting}
Testing the benchmarking suite runs for a while, since each implementation is compiled to verify the functionality of the \textit{Makefiles}. However, this procedure is logged while the unit tests are executed.
	
	
\item Running unit test coverage within the docker container.
\begin{lstlisting}[numbers=none,linewidth=4cm]
./run.sh coverage
\end{lstlisting}
This command measures the coverage of the implemented unit tests. Note that no unit tests are provided for all files that generate output data, since these files change frequently.

\item Opening an interactive bash within the docker container.
\begin{lstlisting}[numbers=none,linewidth=4cm]
./run.sh bash
\end{lstlisting}
This command provides an interactive bash for the docker container. This might be used for debugging reasons and to get a better understanding of the internals of the benchmarking suite.

\end{itemize}